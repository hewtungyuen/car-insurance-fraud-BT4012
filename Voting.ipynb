{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"train.csv\", index_col=False)\n",
    "df_valid = pd.read_csv(\"valid.csv\", index_col = False)\n",
    "df_test = pd.read_csv(\"test.csv\", index_col = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_valid = df_valid.loc[:, df_train.columns]\n",
    "df_test = df_test.loc[:, df_train.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = df_train['FraudFound_P']\n",
    "x_train = df_train.drop('FraudFound_P',axis = 1)\n",
    "y_valid = df_valid['FraudFound_P']\n",
    "x_valid = df_valid.drop('FraudFound_P',axis = 1)\n",
    "y_test = df_test['FraudFound_P']\n",
    "x_test = df_test.drop('FraudFound_P',axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from imblearn.combine import SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=0, sampling_strategy=0.6)\n",
    "\n",
    "x_train, y_train = smote_enn.fit_resample(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.3-py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\new\\lib\\site-packages (from lightgbm) (1.23.4)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\envs\\new\\lib\\site-packages (from lightgbm) (1.8.1)\n",
      "Requirement already satisfied: wheel in d:\\anaconda\\envs\\new\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in d:\\anaconda\\envs\\new\\lib\\site-packages (from lightgbm) (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in d:\\anaconda\\envs\\new\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\envs\\new\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: feature_selection\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.86e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.86e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.714, subsample=1.0 will be ignored. Current value: bagging_fraction=0.714\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.754, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.754\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf1 = KNeighborsClassifier(leaf_size=49, n_neighbors=25, p=1)\n",
    "clf2 = DecisionTreeClassifier(max_depth=6, min_samples_split=8, min_samples_leaf=18,class_weight={0:0.0011, 1: 0.9989})\n",
    "clf3 = GaussianNB()\n",
    "clf4 = LogisticRegression(max_iter=1000)\n",
    "clf5 = LGBMClassifier(lambda_l1 = 2.86e-05,lambda_l2 = 0.754, num_leaves=72, feature_selection = 0.842, bagging_fraction = 0.714, bagging_freq = 2)\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "        ('knn', clf1), ('dt', clf2), ('gnb', clf3), ('lr', clf4), ('lgbm', clf5)], voting='hard')\n",
    "eclf1 = eclf1.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9778    0.7443    0.8452      2131\n",
      "           1     0.1524    0.7313    0.2523       134\n",
      "\n",
      "    accuracy                         0.7435      2265\n",
      "   macro avg     0.5651    0.7378    0.5487      2265\n",
      "weighted avg     0.9290    0.7435    0.8101      2265\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41560644614079734"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = eclf1.predict(x_valid)\n",
    "print(metrics.classification_report(y_valid, y_pred, digits = 4))\n",
    "metrics.fbeta_score(y_valid,y_pred,beta = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9704    0.7231    0.8287      2131\n",
      "           1     0.1285    0.6493    0.2145       134\n",
      "\n",
      "    accuracy                         0.7188      2265\n",
      "   macro avg     0.5495    0.6862    0.5216      2265\n",
      "weighted avg     0.9206    0.7188    0.7924      2265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = eclf1.predict(x_test)\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn import metrics\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# clf1 = KNeighborsClassifier(leaf_size=49, n_neighbors=25, p=1)\n",
    "# clf2 = DecisionTreeClassifier(max_depth=6, min_samples_split=10, min_samples_leaf=8,class_weight={0:0.42, 1: 0.58})\n",
    "# clf3 = GaussianNB()\n",
    "# clf4 = LogisticRegression(max_iter=1000)\n",
    "# eclf1 = VotingClassifier(estimators=[\n",
    "#         ('knn', clf1), ('dt', clf2), ('gnb', clf3), ('lr', clf4)], voting='hard')\n",
    "# eclf1 = eclf1.fit(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NEW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25b5c59147e89bbecdd0cf43bb0f208f4f5e498b6b2f207243061f50280f197a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
